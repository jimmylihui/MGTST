{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# random seed\n",
    "parser.add_argument('--random_seed', type=int, default=2021, help='random seed')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int,  default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str,  default='test', help='model id')\n",
    "parser.add_argument('--model', type=str,  default='MSPTST',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str,  default='ETTh1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='/ssddata/data/jiahuili/PatchTST/all_six_datasets/ETT-small/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=336, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "#parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# PatchTST\n",
    "parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')\n",
    "parser.add_argument('--head_dropout', type=float, default=0.05, help='head dropout')\n",
    "parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n",
    "parser.add_argument('--stride', type=int, default=8, help='stride')\n",
    "parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')\n",
    "parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')\n",
    "parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')\n",
    "parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')\n",
    "parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')\n",
    "parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')\n",
    "parser.add_argument('--individual', type=int, default=0, help='individual head; True 1 False 0')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=16, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=128, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=100, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=10, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type3', help='adjust learning rate')\n",
    "parser.add_argument('--pct_start', type=float, default=0.3, help='pct_start')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "#CARD\n",
    "parser.add_argument('--scale', type=int, default=10, help='scale')\n",
    "parser.add_argument('--gate', type=int, default=0, help='gate')\n",
    "parser.add_argument('--channel_dependent', type=int, default=0, help='gate')\n",
    "args,unkown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MSPTST import Model\n",
    "model=Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): MSPTST_backbone(\n",
       "    (revin_layer): RevIN()\n",
       "    (backbone): ModuleList(\n",
       "      (0): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=80, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=96, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=112, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=144, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=160, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=1936, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (padding_patch_layer): ModuleList(\n",
       "      (0): ReplicationPad1d((0, 8))\n",
       "      (1): ReplicationPad1d((0, 16))\n",
       "      (2): ReplicationPad1d((0, 24))\n",
       "      (3): ReplicationPad1d((0, 32))\n",
       "      (4): ReplicationPad1d((0, 40))\n",
       "      (5): ReplicationPad1d((0, 48))\n",
       "      (6): ReplicationPad1d((0, 56))\n",
       "      (7): ReplicationPad1d((0, 64))\n",
       "      (8): ReplicationPad1d((0, 72))\n",
       "      (9): ReplicationPad1d((0, 80))\n",
       "    )\n",
       "    (identity): Linear(in_features=336, out_features=336, bias=True)\n",
       "    (spatial_backbone): TSTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TSTEncoderLayer(\n",
       "          (self_attn): _MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (sdp_attn): _ScaledDotProductAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (1): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "          (norm_attn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.05, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          )\n",
       "          (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "          (norm_ffn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "        )\n",
       "        (1): TSTEncoderLayer(\n",
       "          (self_attn): _MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (sdp_attn): _ScaledDotProductAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (1): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "          (norm_attn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.05, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          )\n",
       "          (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "          (norm_ffn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "        )\n",
       "        (2): TSTEncoderLayer(\n",
       "          (self_attn): _MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (sdp_attn): _ScaledDotProductAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (1): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "          (norm_attn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.05, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          )\n",
       "          (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "          (norm_ffn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (weight_backbone): ModuleList(\n",
       "      (0): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (1): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.05, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.05, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): ModuleList(\n",
       "      (0): Linear(in_features=16, out_features=42, bias=True)\n",
       "      (1): Linear(in_features=16, out_features=21, bias=True)\n",
       "      (2): Linear(in_features=16, out_features=14, bias=True)\n",
       "      (3): Linear(in_features=16, out_features=10, bias=True)\n",
       "      (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (5): Linear(in_features=16, out_features=7, bias=True)\n",
       "      (6): Linear(in_features=16, out_features=6, bias=True)\n",
       "      (7): Linear(in_features=16, out_features=5, bias=True)\n",
       "      (8): Linear(in_features=16, out_features=4, bias=True)\n",
       "      (9): Linear(in_features=16, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = model.to('cuda:0')\n",
    "model.load_state_dict(torch.load('/ssddata/data/jiahuili/PatchTST/checkpoints/ETTh1.csv_336_96_MSPTST_ETTh1_ftM_sl336_ll0_pl96_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0_scale10_gate0/checkpoint.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "0.5949824\n",
      "0.34518257\n",
      "0.5907598\n",
      "0.30367005\n",
      "0.5054559\n",
      "0.2688641\n",
      "0.17323852\n",
      "(2688, 96, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb22ce49750>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_provider.data_factory import data_provider\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "test_data, test_loader = data_provider(args,flag='test')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "mse=[]\n",
    "preds=[]\n",
    "trues=[]\n",
    "with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "                # (batch_x, batch_y, batch_x_mark, batch_y_mark) =next(iter(test_loader))\n",
    "                \n",
    "                \n",
    "                \n",
    "                batch_x=batch_x.float().to('cuda:0')\n",
    "                batch_y=batch_y.float().to('cuda:0')\n",
    "                outputs = model(batch_x,batch_x_mark,batch_y_mark)\n",
    "                attention_collect=model.get_attn()\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "                pred = outputs.detach().cpu().numpy()  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "                true = batch_y.detach().cpu().numpy()  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "\n",
    "                 \n",
    "                # print(mean_value) \n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "trues = np.concatenate(trues, axis=0)\n",
    "\n",
    "for i in range(7):\n",
    "    mse=np.mean(np.abs(preds[:,:,i] - trues[:,:,i]))\n",
    "    print(mse)\n",
    "fig, axs = plt.subplots(2)\n",
    "print(preds.shape)\n",
    "axs[1].plot(pred[-1,:,3],label='pred')\n",
    "axs[1].plot(true[-1,:,3],label='true')\n",
    "axs[0].plot(np.fft.rfftn(pred[-1,:,3]),label='pred')\n",
    "axs[0].plot(np.fft.rfftn(true[-1,:,3]),label='true')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6d73000517f931139ef55fec43163ce967e53c7b0376fdfcad0ae3b2acbab79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
