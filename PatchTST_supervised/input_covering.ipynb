{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 336 output 96\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# random seed\n",
    "parser.add_argument('--random_seed', type=int, default=2021, help='random seed')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str,  default='ETTh2', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='/ssddata/data/jiahuili/PatchTST/all_six_datasets/ETT-small/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh2.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=336, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "#parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# PatchTST\n",
    "parser.add_argument('--fc_dropout', type=float, default=0.0, help='fully connected dropout')\n",
    "parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')\n",
    "parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n",
    "parser.add_argument('--stride', type=int, default=8, help='stride')\n",
    "parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')\n",
    "parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')\n",
    "parser.add_argument('--affine', type=int, default=1, help='RevIN-affine; True 1 False 0')\n",
    "parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')\n",
    "parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')\n",
    "parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')\n",
    "parser.add_argument('--individual', type=int, default=0, help='individual head; True 1 False 0')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=64, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=128, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=100, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=10, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type3', help='adjust learning rate')\n",
    "parser.add_argument('--pct_start', type=float, default=0.3, help='pct_start')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "parser.add_argument('--scale', type=int, default=10, help='scale')\n",
    "parser.add_argument('--gate',type=int,default=1)\n",
    "parser.add_argument('--channel_dependent', type=int, default=0, help='gate')\n",
    "\n",
    "args,unkown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MSPTST import Model\n",
    "model=Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): MSPTST_backbone(\n",
       "    (revin_layer): RevIN()\n",
       "    (backbone): ModuleList(\n",
       "      (0): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=16, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=48, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=80, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=112, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=144, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=7744, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (padding_patch_layer): ModuleList(\n",
       "      (0): ReplicationPad1d((0, 8))\n",
       "      (1): ReplicationPad1d((0, 16))\n",
       "      (2): ReplicationPad1d((0, 24))\n",
       "      (3): ReplicationPad1d((0, 32))\n",
       "      (4): ReplicationPad1d((0, 40))\n",
       "      (5): ReplicationPad1d((0, 48))\n",
       "      (6): ReplicationPad1d((0, 56))\n",
       "      (7): ReplicationPad1d((0, 64))\n",
       "      (8): ReplicationPad1d((0, 72))\n",
       "      (9): ReplicationPad1d((0, 80))\n",
       "    )\n",
       "    (identity): Linear(in_features=336, out_features=336, bias=True)\n",
       "    (spatial_backbone): TSTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TSTEncoderLayer(\n",
       "          (self_attn): _MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (sdp_attn): _ScaledDotProductAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "          (norm_attn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "          )\n",
       "          (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "          (norm_ffn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "        )\n",
       "        (1): TSTEncoderLayer(\n",
       "          (self_attn): _MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (sdp_attn): _ScaledDotProductAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "          (norm_attn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "          )\n",
       "          (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "          (norm_ffn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "        )\n",
       "        (2): TSTEncoderLayer(\n",
       "          (self_attn): _MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (sdp_attn): _ScaledDotProductAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "          (norm_attn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "          )\n",
       "          (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "          (norm_ffn): Sequential(\n",
       "            (0): Transpose()\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Transpose()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (weight_backbone): ModuleList(\n",
       "      (0): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TSTiEncoder(\n",
       "        (W_P): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (encoder): TSTEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (1): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "            (2): TSTEncoderLayer(\n",
       "              (self_attn): _MultiheadAttention(\n",
       "                (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (sdp_attn): _ScaledDotProductAttention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_attn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "              (ff): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "              )\n",
       "              (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "              (norm_ffn): Sequential(\n",
       "                (0): Transpose()\n",
       "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): Transpose()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=42, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=21, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=14, bias=True)\n",
       "      (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "      (4): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (5): Linear(in_features=64, out_features=7, bias=True)\n",
       "      (6): Linear(in_features=64, out_features=6, bias=True)\n",
       "      (7): Linear(in_features=64, out_features=5, bias=True)\n",
       "      (8): Linear(in_features=64, out_features=4, bias=True)\n",
       "      (9): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = model.to('cuda:0')\n",
    "model.load_state_dict(torch.load('/ssddata/data/jiahuili/PatchTST/checkpoints/336_96_MSPTST_ETTh2_ftM_sl336_ll0_pl96_dm64_nh8_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0/checkpoint.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "0.38284567\n"
     ]
    }
   ],
   "source": [
    "from data_provider.data_factory import data_provider\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import torch.nn as nn\n",
    "test_data, test_loader = data_provider(args,flag='test')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "total_MSE=[]\n",
    "total_MAE=[]\n",
    "MSE=nn.MSELoss()\n",
    "MAE=nn.L1Loss()\n",
    "ratio=0.9\n",
    "\n",
    "preds=[]\n",
    "trues=[]\n",
    "with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "                # (batch_x, batch_y, batch_x_mark, batch_y_mark) =next(iter(test_loader))\n",
    "                \n",
    "                \n",
    "                \n",
    "                batch_x=batch_x.float().to('cuda:0')\n",
    "                batch_y=batch_y.float().to('cuda:0')\n",
    "                # print(batch_x.shape)\n",
    "                mask=torch.rand(batch_x[:,:,:].shape) < ratio\n",
    "                batch_x[:,:,:]=batch_x[:,:,:]*mask.to('cuda:0')\n",
    "                outputs = model(batch_x,batch_x_mark,batch_y_mark)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                pred = outputs[:,:,:].detach().cpu().numpy()  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "                true = batch_y[:,:,:].detach().cpu().numpy()  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "                \n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "                \n",
    "                # total_MSE.append(mae)\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "trues = np.concatenate(trues, axis=0)\n",
    "\n",
    "mse=np.mean(np.abs(preds - trues))\n",
    "print(mse)\n",
    "# plt.plot(pred[3,:,4],label='pred')\n",
    "\n",
    "# plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6d73000517f931139ef55fec43163ce967e53c7b0376fdfcad0ae3b2acbab79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
